from nltk import SnowballStemmer
import nltk
import re
from preprocessing.stopwords import german_stopwords
from typing import List
from nltk.stem import WordNetLemmatizer


class Preprocessor:
    """
    A class that provides text preprocessing and cleaning methods.
    Using the preprocess() method will apply all methods that were activated in the constructor.
    """

    def __init__(self, lower=False, number_removal=False, stopword_removal=False, short_word_removal=False,
                 umlaut_replacing=False, stemming=False, lemmatize=False, n_grams=1, url_email_removal=False,
                 special_character_removal=False,):
        """
        Creates a new Preprocessor object. Using the preprocess() operation of a Preprocessor object will take a text
        as input and create a list of tokens as output. Using the parameter flags preprocessing options can be activated
        or disabled.

        :param lower: whether the preprocessor will convert text to lowercase
        :param number_removal: whether the preprocessor will remove numbers from the text
        :param stopword_removal: whether the preprocessor will remove stopwords
        :param umlaut_replacing: whether the preprocessor will replace umlauts
        :param stemming: whether the preprocessor will perform stemming
        :param n_grams: the number of words per token generated by the preprocessor
        """
        self.lowercase_is_enabled = lower
        self.number_removal_is_enabled = number_removal
        self.stopword_removal_is_enabled = stopword_removal
        self.umlaut_replacing_is_enabled = umlaut_replacing
        self.short_word_removal_is_enabled = short_word_removal
        self.stemming_is_enabled = stemming
        self.lemmatization_is_enabled = lemmatize
        self.n_grams = n_grams
        self.lemmatizer = WordNetLemmatizer() if lemmatize else None
        self.url_email_removal_is_enabled = url_email_removal
        self.special_character_removal_is_enabled = special_character_removal

    def preprocess(self, text: str) -> List[str]:
        """
        Converts the given String into a list of tokens.
        During Conversion the text is cleaned as specified in the constructor.

        | Workflow:
        | 1. Remove URLS and Email addresses from the text,
        | 2. Remove characters used for formatting and layout,
        | 3. Lowercase the text [OPTIONAL],
        | 4. Replace Umlauts [OPTIONAL],
        | 5. Tokenize [Default length = 1, N-Grams can be set via the constructor],
        | 6. Remove punctuation tokens
        | 7. Remove alphanumeric tokens [OPTIONAL]
        | 8. Remove stop words [OPTIONAL]
        | 9. Stem OR lemmatize tokens [OPTIONAL]
        | 10. Remove short (less than 3 characters) words [OPTIONAL]

        :param text: a String representing a single document
        :return: A list of tokens
        """

        if self.url_email_removal_is_enabled:
            text = Preprocessor.remove_urls(text)
            text = Preprocessor.remove_emails(text)

        if self.special_character_removal_is_enabled:
            text = Preprocessor.remove_special_symbols(text)

        if self.lowercase_is_enabled:
            text = self.lowercase(text)
        if self.umlaut_replacing_is_enabled:
            text = self.replace_umlauts(text)

        tokens = Preprocessor.tokenize(text, self.n_grams)
        tokens = Preprocessor.remove_punctuation_tokens(tokens)

        if self.number_removal_is_enabled:
            tokens = Preprocessor.remove_alphanumeric_tokens(tokens)

        if self.stopword_removal_is_enabled:
            tokens = self.remove_stop_words(tokens)
        if self.stemming_is_enabled:
            tokens = self.stem(tokens)
        elif self.lemmatization_is_enabled and not self.stemming_is_enabled:
            tokens = self._lemmatize(tokens)
        if self.short_word_removal_is_enabled:
            tokens = self.remove_short_words(tokens, 3)

        return tokens

    @staticmethod
    def remove_urls(text: str) -> str:
        """
        Removes all urls from the given text.

        :param text: the text from which the urls will be removed
        :return: the text without urls
        """
        return re.sub(r'(([hH])ttps?://|www.)\S*', '', text)

    @staticmethod
    def remove_emails(text: str) -> str:
        """
        Removes all email addresses from the given text.

        :param text: the text from which the email addresses will be removed
        :return: the text without email addresses
        """
        return re.sub(r'\S+@\S+\.\S+', '', text)

    @staticmethod
    def remove_special_symbols(text: str) -> str:
        """
        Removes special characters from the given text. Here special characters are symbols used for layouting
        and formatting: #*＊+~-⁃–—_<=>/|\\•・▪■●►◆◈◇◉◊○◯※‘^`'’“„”»«›‹€£$%&§®©™@【】▼▷▶▬≡…√"

        :param text: the text from which special characters will be removed
        :return: the text without special characters
        """
        special_symbols = "#*＊+~-⁃–—_<=>/|\\•・▪■●►◆◈◇◉◊○◯※‘^`'’“„”»«›‹€£$%&§®©™@【】▼▷▶▬≡…√" + '"'
        for symbol in special_symbols:
            text = text.replace(symbol, " ")
        return text

    @staticmethod
    def lowercase(text: str) -> str:
        """
        Converts every character in the given text to lowercase.

        :param text: the given text
        :return: the text in lowercase
        """
        return text.lower()

    @staticmethod
    def tokenize(text: str, n: int = 1) -> List[str]:
        """
        Splits a text into a list of word n-grams, where n is the number of words per token. Here whitespaces are
        used to determine the boundaries between words.
        Using a 1-gram will result in a list of single words, while using 2-grams will result in a list with two
        adjacent words for each entry separated by one whitespace,
        e.g. ["word1 word2", "word2 word3", "word3 word4", ...] and so on.
        If the given n-gram length is greater than the total amount of words in the provided text, the n-gram length
        will default to the length of the text. This will result in a list with its only entry being the full text.

        :param text: the text that will be split into tokens
        :param n: the number of words per token
        :return: a list of Strings
        """
        if n == 1:
            return nltk.tokenize.word_tokenize(text, 'german')

        # Reduce multiple whitespaces to one space and use the remaining spaces to split into a list of words
        tokens = ' '.join(text.split()).split()

        if n > len(tokens):
            n = len(tokens)

        if n > 1:
            n_gram_tokens = [" ".join(tokens[i:i + n]) for i in range(len(tokens) - n + 1)]
            return n_gram_tokens

    @staticmethod
    def remove_punctuation_tokens(tokens: List[str]) -> List[str]:
        """
        Removes all punctuation symbols from the given list of tokens. Punctuation symbols: .!?:;,/[](){}

        :param tokens: a list of tokens
        :return: the list without punctuation tokens
        """
        punctuation = ".!?:;,/[](){}"
        tokens = [t for t in tokens if t not in punctuation]
        return tokens

    @staticmethod
    def remove_alphanumeric_tokens(tokens: List[str]) -> List[str]:
        """
        Removes all tokens from the given list that contain numbers. Leaving only purely alphabetic characters.

        :param tokens: a list of tokens
        :return: the list without alphanumeric tokens
        """
        regex = re.compile(r'^\S*\d+\S*$')
        filtered = [t for t in tokens if not regex.match(t)]
        return filtered

    @staticmethod
    def remove_stop_words(tokens: List[str]) -> List[str]:
        """
        Removes all words that are considered german stopwords from the given list of tokens. Stopwords are words that
        rarely carry any information about the content of document and tend to appear in most documents very frequently.

        :param tokens: a list of tokens
        :return: the list without german stopwords
        """
        stopwords = german_stopwords
        return [t for t in tokens if t.lower() not in stopwords]

    @staticmethod
    def stem(tokens: List[str]) -> List[str]:
        """
        Converts a list of tokens into a list of their word stems.
        Uses the nltk implementation which has inbuilt lowercasing. This means the resulting tokens will be in
        lowercase, regardless of the input and possible Constructor settings.

        :param tokens: a list of tokens
        :return: a list of lowercase, stemmed tokens
        """
        stemmer = SnowballStemmer("english")
        stemmed_tokens = []
        for token in tokens:
            stemmed_tokens.append(stemmer.stem(token))
        return stemmed_tokens

    @staticmethod
    def lemmatize(tokens: List[str]) -> List[str]:
        lemmatizer = WordNetLemmatizer()
        lemmatized_tokens = []
        for t in tokens:
            pos = nltk.pos_tag([t])[0][1]
            verbs = ['VBZ', 'VBD', 'VBP', 'MD']
            nouns = ['NN', 'NNS']
            adj = ['JJ', 'RBR']
            if pos in verbs:
                pos = "v"
            elif pos in adj:
                pos = "a"
            elif pos in nouns:
                pos = "n"
            else:
                pos = "n"
            lemma = lemmatizer.lemmatize(t, pos=pos)
            lemmatized_tokens.append(lemma)
        return lemmatized_tokens

    def _lemmatize(self, tokens: List[str]) -> List[str]:
        if self.lemmatizer is None:
            self.lemmatizer = WordNetLemmatizer()
        lemmatized_tokens = []
        for t in tokens:
            pos = nltk.pos_tag([t])[0][1]
            verbs = ['VBZ', 'VBD', 'VBP', 'MD']
            nouns = ['NN', 'NNS']
            adj = ['JJ', 'RBR']
            if pos in verbs:
                pos = "v"
            elif pos in adj:
                pos = "a"
            elif pos in nouns:
                pos = "n"
            else:
                pos = "n"
            lemma = self.lemmatizer.lemmatize(t, pos=pos)
            lemmatized_tokens.append(lemma)
        return lemmatized_tokens

    @staticmethod
    def remove_short_words(tokens: List[str], min_length: int) -> List[str]:
        """
        Removes every word that is shorter than the given minimum length from the text.

        :param tokens: a list of Strings
        :param min_length: the minimum length for a word in the token list
        :return: a list of Strings only containing tokens of a minimum length
        """
        return [t for t in tokens if len(t) >= min_length]

    @staticmethod
    def replace_umlauts(text: str) -> str:
        """
        Replaces the umlauts in the given Text: ä -> ae, ö -> oe, ü -> ue, Ä -> Ae, Ö -> Oe, Ü -> Ue,
        also replaces ß with 'ss'

        :param text: the processed text
        :return: the text without umlauts
        """
        umlauts = {'ä': 'ae', 'ö': 'oe', 'ü': 'ue', 'Ä': 'Ae', 'Ö': 'Oe', 'Ü': 'Ue', 'ß': 'ss'}
        for u in umlauts:
            text = text.replace(u, umlauts[u])
        return text
